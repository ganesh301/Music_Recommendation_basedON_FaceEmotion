{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "kIDyKlHE9uuV"
   },
   "outputs": [],
   "source": [
    "def stringToList(input):\n",
    "    lst = []\n",
    "    for i in input:\n",
    "        # Extract values within the square brackets and split by \",\"\n",
    "        values_str = i[i.find('[')+1:i.find(']')].split(',')\n",
    "        \n",
    "        # Convert to float\n",
    "        values = list(map(float, values_str))\n",
    "        lst.append(values)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bvfCPgPJ9vci"
   },
   "outputs": [],
   "source": [
    "def moodNamePrintFromLabel(n):\n",
    "  if n == 0: result = 'Angry '\n",
    "  elif n == 1: result = 'Disgust '\n",
    "  elif n == 2: result = 'Fear'\n",
    "  elif n == 3: result = 'Happy'\n",
    "  elif n == 4: result = 'Sad'\n",
    "  elif n == 5: result = 'Surprise'\n",
    "  elif n == 6: result = 'Neutral'\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfISzA7WxD5e",
    "outputId": "4a9c6c43-7bda-4577-95ed-5d95fd711e84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "_O6gXlIqVsRv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "NikBvZDmxLEo"
   },
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('D:\\Music-recommendation-based-on-facial-emotion-recognition-main (1)\\Music-recommendation-based-on-facial-emotion-recognition-main\\Image_data\\\\train.csv')\n",
    "test_data = pd.read_csv('D:\\Music-recommendation-based-on-facial-emotion-recognition-main (1)\\Music-recommendation-based-on-facial-emotion-recognition-main\\Image_data\\\\test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nku8k1fHQmqI"
   },
   "source": [
    "**processing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "2yeacQwxxOuh"
   },
   "outputs": [],
   "source": [
    "X = data['pixels']\n",
    "Y = np.array(data['emotion'])\n",
    "\n",
    "x_test_data = np.array(test_data['pixels'])\n",
    "\n",
    "X = np.array(stringToList(X))/255.0\n",
    "\n",
    "X = np.reshape(X, (len(X), 48, 48, 1))  # Adjusted the reshape size to len(X)\n",
    "\n",
    "#x_test_data = np.array(stringToList(x_test_data))\n",
    "x_test_data = np.array(stringToList(x_test_data))/255.0\n",
    "x_test_data = np.reshape(x_test_data, (len(x_test_data), 48, 48, 1))  # Adjusted the reshape size to len(x_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6y7TbxXzhOn",
    "outputId": "ae54c71b-219c-409b-aa84-07ada70cdc39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16804, 48, 48, 1)\n",
      "(16804,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh9PCf9qQtTJ"
   },
   "source": [
    "**data splitting to test and training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wmsgFrG-yzzf"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TrHAPy-0jsJ",
    "outputId": "f33281bc-7dfa-4cb4-ef0f-36c94cd02e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13443, 48, 48, 1)\n",
      "(13443,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "s9-N9GxExSpg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, Dropout, MaxPooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b2f7xg2MQ9th"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From D:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "251/379 [==================>...........] - ETA: 21s - loss: 1.6093 - sparse_categorical_accuracy: 0.3142"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 69\u001b[0m\n\u001b[0;32m     64\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Activation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     66\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m               optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     68\u001b[0m               metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 69\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#1st convo\n",
    "model.add(Conv2D(96, (3,3), input_shape = (48,48,1)))\n",
    "model.add(Activation('relu'))\n",
    "#polling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "#2nd convo\n",
    "model.add(Conv2D(256, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "#polling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#3rd convo\n",
    "model.add(Conv2D(384, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "#polling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#4th convo\n",
    "model.add(Conv2D(256, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "#polling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#passing through dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "#1st dense layer\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "#dropout\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "#2nd dense layer\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "#dropout\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "#3rd dense layer\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "#dropout\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = ['sparse_categorical_accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "muYX6STVBug2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('D:\\\\Music-recommendation-based-on-facial-emotion-recognition-main (1)\\\\Music-recommendation-based-on-facial-emotion-recognition-main\\\\Image_data\\\\model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-LAC-jGQ4qa"
   },
   "source": [
    "**Loading the saved mood detection model from google drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "G2v_p5rLxVLw"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model('D:\\\\Music-recommendation-based-on-facial-emotion-recognition-main (1)\\\\Music-recommendation-based-on-facial-emotion-recognition-main\\\\Image_data\\\\model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ONLK6a52Tnl",
    "outputId": "05b0f053-782a-4e75-feb0-6feea4d1722e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 5s 45ms/step - loss: 0.5622 - sparse_categorical_accuracy: 0.8021\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# ... (rest of your imports)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "def moodNamePrintFromLabel(n):\n",
    "    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    return emotions[n]\n",
    "\n",
    "# ... (rest of your code)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "video_cap = cv2.VideoCapture(0)  # Open the webcam\n",
    "\n",
    "while (video_cap.isOpened()):\n",
    "    ret, frame = video_cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame\")\n",
    "        break\n",
    "\n",
    "    gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_img, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        pxl_lst = []\n",
    "        for i in range(y, y + h):\n",
    "            lst = []\n",
    "            for j in range(x, x + w):\n",
    "                lst.append(gray_img[i][j])\n",
    "            pxl_lst.append(lst)\n",
    "        single_face = np.array(pxl_lst)\n",
    "\n",
    "        resized_img = cv2.resize(single_face, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "        resized_img = np.reshape(resized_img, (1, 48, 48, 1)) / 255.0\n",
    "\n",
    "        # Get the result from the model\n",
    "        result = np.argmax(loaded_model.predict(resized_img), axis=-1)\n",
    "\n",
    "        # Draw a rectangle around the face and display the mood label\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, moodNamePrintFromLabel(result[0]), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame with mood information\n",
    "    cv2.imshow('Webcam Mood Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "video_cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# Call the function\n",
    "#videoToMoodDetection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdw4eRdtAxlG"
   },
   "source": [
    "**Taking input as image and videos and detecting faces and getting the face portion. Then passing these faces to the model and getting the mood detection output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "lS_uk9L5H_it"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def imageToMoodDetection(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_img, 1.3, 3)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        pxl_lst = []\n",
    "        for i in range(y, y + h):\n",
    "            lst = []\n",
    "            for j in range(x, x + w):\n",
    "                lst.append(gray_img[i][j])\n",
    "            pxl_lst.append(lst)\n",
    "        single_face = np.array(pxl_lst)\n",
    "        single_face = np.reshape(single_face, (h, w,))\n",
    "\n",
    "        resized_img = cv2.resize(single_face, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "        resized_img = np.reshape(resized_img, (1, 48, 48, 1)) / 255.0\n",
    "\n",
    "        # Display the resized image using matplotlib\n",
    "        plt.imshow(resized_img[0, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Resized Image Shape:\", resized_img.shape)\n",
    "\n",
    "        result = np.argmax(loaded_model.predict(resized_img), axis=-1)\n",
    "        print(\"Predicted Mood:\", moodNamePrintFromLabel(result))\n",
    "\n",
    "# Call the function with the image path\n",
    "imageToMoodDetection('D:\\\\Music-recommendation-based-on-facial-emotion-recognition-main (1)\\\\Music-recommendation-based-on-facial-emotion-recognition-main\\\\Image_data\\\\test\\\\disgusted\\\\im1.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "Y6p4OgKW-dlP",
    "outputId": "2054e819-361d-4300-8727-cea654f06e27"
   },
   "outputs": [],
   "source": [
    "imageToMoodDetection('D:\\\\Music-recommendation-based-on-facial-emotion-recognition-main (1)\\\\Music-recommendation-based-on-facial-emotion-recognition-main\\\\Image_data\\\\test\\\\disgusted\\\\im1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zvNVY6E383EU",
    "outputId": "661fa2bb-56ec-4a28-babd-b8b19c917b48"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFRa-zu9xfw_"
   },
   "source": [
    "**pixel to image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "_eAaZJeKxc7g",
    "outputId": "beea6d64-0bb9-4437-b65a-1b3d4d128a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 1)\n",
      "(48, 48)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24c5c73d610>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "single_image = np.array(x_test_data[13]*255)\n",
    "print(single_image.shape)\n",
    "single_image = np.reshape(single_image,(48,48),order = 'C')\n",
    "print(single_image.shape)\n",
    "plt.imshow(single_image, cmap='gray', vmin=0, vmax=255)\n",
    "# s_img = np.array(single_image, shape=(48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNlHHpVLr4kkkQLflwds7it",
   "collapsed_sections": [],
   "name": "mood detection from videos and images.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
